[training_param]
batch_size = 32
seed = 0
train_iter = 600000
learning_rate = 0.0025
gpus = main = 0, slave = 1
n_processes = 16
init_learning_rate = 0.01
# `2`, `3` or `2,3`. The last option train both 2 and 3 dim
joint_dims = 2,3
# input image size WxH
imsize = 224x224
# input cube DxHxW
cube = 200x200x200
# augmentation parameter
## flip
enable_x_flip = false
enable_y_flip = false
## rotate
angle_range = -90,90
## crop: np.arange(begin,end,step)
scale_range = 1.0, 1.5, 0.01
shift_range = -0.15, 0.15, 0.01


[result]
dir = /work/trained

# select dataset
# ConfigParser.getboolean is case-insensitive and recognizes
# Boolean values from
# from `yes`/`no`, `on`/`off`, `true`/`false` and `1`/`0`
[dataset]
train_set = fhad,stb,multiview,freihand
val_set = fhad,stb,multiview,freihand
test_set = stb
# specify color(rgb) or depth image on training.
use_rgb = yes
use_depth = no

[dataset_dir]
# specify dataset directory of your host machine.
# First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations
fhad = ~/dataset/fhad
# Stereo Tracking Benchmark Dataset (STB)
stb = ~/dataset/stb
# RHD
rhd = ~/dataset/RHD_published_v2
# GANerated dataset
ganerated = ~/dataset/GANeratedHands_Release
# Synth Hands dataset
synth = ~/dataset/nyu_hand_dataset_v2
# multiview
multiview = ~/dataset/multiview_hand
# FreiHAND
freihand = ~/dataset/FreiHAND_pub_v1

# select model
[model]
name = ppn

[ppn]
feature_extractor = mv2

# MobileNetV2
[mv2]
width_multiplier = 1.0

# ResNet
[resnet]
# must be `18`, `34` or `50`
n_layers = 18

