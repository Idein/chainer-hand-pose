[training_param]
# batchsize / num of gpus equals the batchsize per gpu
batchsize = 64
learning_rate = 0.001
gpus = main=0, slave1=1
num_process = 16
seed = 0
train_iter = 200000
schedule = 150000


# input file or dir path in docker container environment
# select dataset
# ConfigParser.getboolean is case-insensitive and recognizes
# Boolean values from
# from `yes`/`no`, `on`/`off`, `true`/`false` and `1`/`0`
[dataset]
enable_x_flip = yes
enable_y_flip = no
angle_range = -30,30
train_set = rhd,stb
val_set = rhd,stb
test_set = stb

[dataset_dir]
# First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations
fhad = ~/dataset/fhad
# Stereo Tracking Benchmark Dataset (STB)
stb = ~/dataset/stb
# RHD
rhd = ~/dataset/RHD_published_v2
# GANerated dataset
ganerated = ~/dataset/GANeratedHands_Release
# Synth Hands dataset
synth = ~/dataset/nyu_hand_dataset_v2
# multiview
multiview = ~/dataset/multiview_hand
# FreiHAND
freihand = ~/dataset/FreiHAND_pub_v1

[output_path]
result_dir = /work/trained

[model_param]
model_path = models/hand_ssd_mobilenet_v2
feature_extractor = MobileNetV2
ssd_extractor = MobileNetV2LiteExtractor300
input_size = 256
num_layers = 6
# must be
#`hand_class=left, right`
# or
#`hand_class=hand`
hand_class = left, right
smin = 0.025
smax = 0.8
width_multiplier = 1.0
resolution_multiplier = 1.0



